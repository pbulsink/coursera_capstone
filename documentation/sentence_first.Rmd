---
title: "Sentence then token"
author: "Philip Bulsink"
date: '2017-04-07'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(quanteda)
library(stringr)
```

##Read in datat

```{r}
library(readr)
en_blogs<-read_lines("./data/en_US/en_US.blogs.txt", progress=FALSE)
en_news<-read_lines("./data/en_US/en_US.news.txt", progress=FALSE)
en_twitter<-read_lines("./data/en_US/en_US.twitter.txt", progress=FALSE)
blogs_lines<-length(en_blogs)
news_lines<-length(en_news)
twitter_lines<-length(en_twitter)
```

Split to sets: dev, training, test.
```{r}
set.seed(1)
fraction_dev<-0.05
fraction_training<-0.65
fraction_test<-0.30
blog_role<-sample(1:3, size = length(en_blogs), replace = TRUE, prob = c(fraction_dev, fraction_training, fraction_test))
news_role<-sample(1:3, size = length(en_blogs), replace = TRUE, prob = c(fraction_dev, fraction_training, fraction_test))
twitter_role<-sample(1:3, size = length(en_blogs), replace = TRUE, prob = c(fraction_dev, fraction_training, fraction_test))
blog_dev<-en_blogs[blog_role == 1]
news_dev<-en_news[news_role == 1]
twitter_dev<-en_twitter[twitter_role == 1]
```

Hack each to sentences.
```{r}
library(quanteda)
library(stringi)

blog_dev<-as.character(tokens(blog_dev, what='sentence', removeNumbers=TRUE, removePunc=TRUE, removeSymbols=TRUE, removeSeparators=TRUE, removeTwitter=TRUE, removeHyphens=TRUE, removeURL=TRUE))

news_dev<-as.character(tokens(news_dev, what='sentence', removeNumbers=TRUE, removePunc=TRUE, removeSymbols=TRUE, removeSeparators=TRUE, removeTwitter=TRUE, removeHyphens=TRUE, removeURL=TRUE))

twitter_dev<-as.character(tokens(twitter_dev, what='sentence', removeNumbers=TRUE, removePunc=TRUE, removeSymbols=TRUE, removeSeparators=TRUE, removeTwitter=TRUE, removeHyphens=TRUE, removeURL=TRUE))

dev_tot<-c(blog_dev, news_dev, twitter_dev)
#rm(blog_dev, news_dev, twitter_dev)

dev_tot<-stri_trans_general(dev_tot,"Latin-ASCII")
dev_tot <- gsub("[µºˆ_]+", '', dev_tot)

#set to lowercase
dev_tot <- char_tolower(dev_tot)

#Remove URLS
dev_tot <- gsub("(f|ht)tp(s?):\\/\\/(.*)[.][a-z=\\?\\/]+", "", dev_tot)

#Remove Twitter 'Via @username' and '@usernames' and 'rt' and hashtags
dev_tot <- gsub("via\\s*@[a-z_]{1,20}","", dev_tot)
dev_tot <- gsub("\\s@[a-z_]{1,20}", "", dev_tot)
dev_tot <- gsub("\\brt\\b", "", dev_tot)
dev_tot <- gsub("#[a-z0-9_]+", "", dev_tot)

#Cut punctuation/symbols/numbers
dev_tot <- gsub("[^a-zA-Z\\ ]", "", dev_tot)

#Get a 'bad words' list to remove from the corpus
badwords_url <- "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
badwords <- unlist(strsplit(RCurl::getURL(badwords_url), "\n"))
#last 'bad word' is an emoji, remove
badwords <- badwords[1:length(badwords)-1]
#replacement builder code from getAnywhere(tm::removeWords.character)
dev_tot <- gsub(sprintf("(*UCP)\\b(%s)\\b", paste(badwords, collapse = "|")), "", dev_tot, perl = TRUE)

#Remove multiple spaces from deleted words
dev_tot <- gsub("\\s\\s+", " ", dev_tot)

dfm1<-dfm(dev_tot, removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE, removeSeparators = TRUE, removeTwitter = TRUE, removeHyphens = TRUE, removeURL = TRUE, ngrams = 1)

#add start ^ and end $ tags to the sentences.
dev_tot<-paste("^",dev_tot,"$", sep=" ")

dfm2<-dfm(dev_tot, removeNumbers = FALSE, removePunct = FALSE, removeSymbols = FALSE, removeSeparators = TRUE, removeHyphens = FALSE, removeURL = FALSE, ngrams = 2)
dfm3<-dfm(dev_tot, removeNumbers = FALSE, removePunct = FALSE, removeSymbols = FALSE, removeSeparators = TRUE, removeHyphens = FALSE, removeURL = FALSE, ngrams = 3)
dfm4<-dfm(dev_tot, removeNumbers = FALSE, removePunct = FALSE, removeSymbols = FALSE, removeSeparators = TRUE, removeHyphens = FALSE, removeURL = FALSE, ngrams = 4)
```

```{r n_gram}
library(stringr)

splitGrams<-function(docfeatmatrix){
    freq_table<-colSums(docfeatmatrix)
    grams<-colnames(docfeatmatrix)
    grams<-trimws(gsub("_", " ", grams))
    n<-length(strsplit(grams[1], split = ' ')[[1]])
    if (n > 1){
        grams<-str_split_fixed(grams, ' ', n)
        gram_table<-data.frame(pregram=apply(grams, 1, function(x) paste(x[1:n-1], collapse = '_')), postgram=grams[,n], count=freq_table, stringsAsFactors = FALSE)
    }
    else if (n == 1){
        gram_table<-data.frame(pregram = rep(''), postgram=grams, count = freq_table, stringsAsFactors = FALSE)
    }
    else{
        stop("n-gram input not valid: < 1 n-gram length")
    }
    rm(grams, freq_table, n)
    #fix ordering
    gram_table<-gram_table[order(gram_table$pregram, -gram_table$count, gram_table$postgram),]
    rownames(gram_table)<-NULL
    return(gram_table)
}

gt1<-splitGrams(dfm1)
gt2<-splitGrams(dfm2)
gt3<-splitGrams(dfm3)
gt4<-splitGrams(dfm4)
```

```{r}
library(pryr)
```


R lookups of very large data.frames (> 100 million rows) can be slow. For data analysis used for scholarly work or commercial interest, this may be acceptable. However, for an interactive or near-real-time analysis, this is not acceptable. 

In many languages exists a dictionary type object, referencing values by a set unique key. There may be hashing or other techniques running in the background providing this speed. For example, Python dictionary lookups are O(1), while list lookups are O(n). The tradeoff is that dictionaries take more storage space, as the system keeps the hash table only 2/3 full (many references, see: [https://stackoverflow.com/questions/513882/python-list-vs-dict-for-look-up-table]). R does not contain a built in dictionary type, but there is a workaround.

Much of this work is exploring work done by others, such as Jeffrey Horner, and not my own. See [Jeffrey's posts] (http://jeffreyhorner.tumblr.com/post/116325104028/hash-table-performance-in-r-part-ii-in-part-i) about this topic for a better explanation of what is happening.

In the Coursera Data Science Specialization, a significant number of n-grams are produced and stored for use in a prediction model. This needs to be able to look up results quickly, even if the table contains millions of pre-tested n-grams. Having a fast lookup increases the size of the n-gram table we can use, and also allows for n+m-grams (instead of being limited to bigram/trigram). 

For this purpose, the following storage solution is proposed:

| Key | Value |
|---|---|
| `pregram` | `list(postgram = frequency, postgram = frequency, ...)` |
| `go_to_the` | `list('gym'=12, 'beach'=10, 'hospital'=8, 'store'=8)` |

where pregram, postgram, are counts, and frequency is the relative appearance.

R allows for the creation of a hashed environment, where you can put and retrieve items. This is easier to do than it sounds.

```{r}
HASH_TABLE <- function() new.env()
INSERT <- function(key, value, ht)  ht[[key]] <- value
LOOKUP <- function(key, ht) ht[[key]]
DELETE <- function(key, ht) rm(list=key,envir=ht,inherits=FALSE)
```

Now, all we need to do is to use these `INSERT`, `LOOKUP`, and (less frequently) `DELETE` functions on our new `HASH_TABLE`.

For example, I'll load a smallish gram table from before:
```{r}
#gt2<-readRDS("./data/en_gt2.RDS")
```

We can replace the gram table data.frame with an environment table:
```{r}
ht2<-HASH_TABLE()
# buildGTHash<-function(gt, ht){
#     n<-unique(gt$pregram)
#     pb<-dplyr::progress_estimated(length(n))
#     for(i in 1:length(n)){
#         k<-n[i]
#         x<-gt[gt$pregram == k,]
#         v<-x[,'count']
#         names(v)<-x[,'postgram']
#         INSERT(k, v, ht)
#         pb$tick()$print()
#     } 
# }

GTHash<-function(n, gt, ht){
    x<-gt[gt$pregam == n,]
    v<-x[,'count']
    names(v)<-x[,'postgram']
    INSERT(n,v,ht)
    return(TRUE)
}
vGTHash<-Vectorize(FUN = GTHash, vectorize.args = 'n', SIMPLIFY = TRUE, USE.NAMES = FALSE)

#USAGE: vGTHash(unique(gt$pregram), gt, ht)
vGTHash(unique(gt2$pregram), gt2, ht2)
```

Likewise:
```{r ht3}
ht3<-HASH_TABLE()
vGTHash(unique(gt3$pregram), gt3, ht3)

ht4<-HASH_TABLE()
vGTHash(unique(gt4$pregram), gt4, ht4)
```
