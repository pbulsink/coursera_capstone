---
title: "Data Exploration and Milestone Report - Coursera Data Science Capstone"
author: "Phil Bulsink"
date: "May 15, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(syuzhet)
library(ggplot2)
library(quanteda)
library(stringi)
library(stringr)
library(ggplot2)
library(knitr)
```

The Coursera Data Science program has a final major capstone project, where we use provided data to produce predictions akin to those found on cell phone keyboards. This predicts the next word based on the previous word(s) typed by the user. 

This first report outlines the data exploration and initial development performed for this purpose

#Data Acquisition
Data can be downloaded from the following source and unpacked for analysis. The download is quite large, and takes significant space when unpacked, so chose a working directory suitable for that purpose.

```{r getfiles, echo=TRUE, eval=FALSE}
#NOT RUN
url<-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(url, "./Coursera-SwiftKey.zip")
unzip("./Coursera-SwiftKey.zip", exdir = ".")
dir.create("./data")
file.rename("./final/", "./data/")
```

With the file downloaded and extracted, we can see that it has three large English text files (as well as Russian, Finish, and German language files). We'll leave them, it could be interesting to play with them later. These files are compiled blog posts, news reports, and Twitter tweets 

Our text files are quite big:
```{r filesize}
#file.size/1000000 to get MB
fs<-file.size(c("./data/en_US/en_US.blogs.txt", "./data/en_US/en_US.news.txt", "./data/en_US/en_US.twitter.txt"))/1000000
fs
```

So with three files near 200 MB each, we should perform some sub-setting for our exploratory work. We'll read each file, then see how many lines they have. The [readr](https://cran.r-project.org/package=readr) package makes this easier.
```{r file_explore}
library(readr)
en_blogs<-read_lines("./data/en_US/en_US.blogs.txt", progress=FALSE)
en_news<-read_lines("./data/en_US/en_US.news.txt", progress=FALSE)
en_twitter<-read_lines("./data/en_US/en_US.twitter.txt", progress=FALSE)
blogs_lines<-length(en_blogs)
news_lines<-length(en_news)
twitter_lines<-length(en_twitter)
```

So, our blog file has `r blogs_lines` lines, the news file has `r news_lines` lines, and the twitter file has `r twitter_lines` lines. There's likely a difference in the number of words in each of the lines of the files, a quick example could be twitter (where no line should be above 140 characters) and news or blogs. We'll investigate these distributions. 

```{r words}
en_blogs_word<-strsplit(gsub("[[:punct:]]","",en_blogs), " ")
en_news_word<-strsplit(gsub("[[:punct:]]","",en_news), " ")
en_twitter_word<-strsplit(gsub("[[:punct:]]","",en_twitter), " ")
blogs_wordcount<-length(unlist(en_blogs_word))
blogs_un_wordcount<-length(unique(unlist(en_blogs_word)))
news_wordcount<-length(unlist(en_news_word))
news_un_wordcount<-length(unique(unlist(en_news_word)))
twitter_wordcount<-length(unlist(en_twitter_word))
twitter_un_wordcount<-length(unique(unlist(en_twitter_word)))
```

A histogram of each file's word per line count:
```{r hist_prep, include=FALSE}
gc(verbose = FALSE)
bl_wd<-data.frame("File"="blog", "words"=sapply(en_blogs_word, length))
rm(en_blogs_word)
nw_wd<-data.frame("File"="news", "words"=sapply(en_news_word, length))
rm(en_news_word)
tw_wd<-data.frame("File"="twitter","words"=sapply(en_twitter_word, length))
rm(en_twitter_word)
words<-rbind(bl_wd, nw_wd, tw_wd)
rm(bl_wd, nw_wd, tw_wd)
gc(verbose = FALSE)
words$File<-as.factor(words$File)
g<-ggplot(words, aes(words, color = File, fill = File)) + geom_density(alpha = 0.1) + xlim(c(0,150))
rm(words)
```

```{r hist}
g
```

A summary of all of the discovoered data is below:
```{r table_prep, include=FALSE}
sumdata<-data.frame("File" = c("Blogs", "News", "Twitter"), "File.Size"=c(fs[1], fs[2], fs[3]), 
                    "Lines"=c(blogs_lines, news_lines, twitter_lines),
                    "Words" = c(blogs_wordcount, news_wordcount,twitter_wordcount), 
                    "Unique.Words" = c(blogs_un_wordcount, news_un_wordcount, twitter_un_wordcount))
sumdata$Prop.Unique<-sumdata$Unique.Words/sumdata$Words
#kable(sumdata, caption = "Summary of File Info")
```

```{r table, echo=FALSE}
kable(sumdata, caption = "Summary of file information")
```

For the rest of our exploration we'll use only 5% of each file, randomly selected, to keep computation cost down. We'll look at each file separately as well, to see if there's any differences.

```{r subset}
set.seed(1)
fraction<-0.05
en_blogs<-en_blogs[sample(c(1:blogs_lines), blogs_lines*fraction)]
en_news<-en_news[sample(c(1:news_lines), news_lines*fraction)]
en_twitter<-en_twitter[sample(c(1:twitter_lines), twitter_lines*fraction)]
```

#Data Exploration
One of the things we will be doing is building 'n-gram' models. However, they should be built once the data is cleaned. To clean the data effectively, we'll use the [`quanteda`](https://cran.r-project.org/package=quanteda) package in R. We'll also use [`stringi`](https://cran.r-project.org/package=stringi) to clear any string formatting issues. We'll clean, then discuss the cleaning and compare to other language processing that may be utilized in other situations.

```{r cleanup_example}
library(quanteda)
library(stringi)

#stokens<-tokens(c_blogs, what='sentence', removeNumbers=TRUE, removePunc=TRUE, removeSymbols=TRUE, removeSeparators=TRUE, removeTwitter=TRUE, removeHyphens=TRUE, removeURL=TRUE)

#Prepare the text data
#Fix accented characters (Not applicable for other languages) & some symbols associated
en_blogs <- stri_trans_general(en_blogs,"Latin-ASCII")
en_blogs <- gsub("[µºˆ_]+", '', en_blogs)

#set to lowercase
en_blogs <- char_tolower(en_blogs)

#Remove URLS
en_blogs <- gsub("(f|ht)tp(s?):\\/\\/(.*)[.][a-z=\\?\\/]+", "", en_blogs)

#Remove Twitter 'Via @username' and '@usernames' and 'rt' and hashtags
en_blogs <- gsub("via\\s*@[a-z_]{1,20}","", en_blogs)
en_blogs <- gsub("\\s@[a-z_]{1,20}", "", en_blogs)
en_blogs <- gsub("\\brt\\b", "", en_blogs)
en_blogs <- gsub("#[a-z0-9_]+", "", en_blogs)

#Get a 'bad words' list to remove from the corpus
badwords_url <- "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
badwords <- unlist(strsplit(RCurl::getURL(badwords_url), "\n"))
#last 'bad word' is an emoji, remove
badwords <- badwords[1:length(badwords)-1]
#replacement builder code from getAnywhere(tm::removeWords.character)
en_blogs <- gsub(sprintf("(*UCP)\\b(%s)\\b", paste(badwords, collapse = "|")), "", en_blogs, perl = TRUE)

#gsub(sprintf("(*UCP)\\b(%s)\\b", paste(stopwords("en"), collapse = "|")), "", en_blogs, perl = TRUE)
```

So what all happened there? We converted all of the text to normal ASCII characters (not applicable for languages with accented characters), then to lower case, then took out URLs and bad words. We removed some Twitter noise (usernames, 'via @usernames', hashtags, and 'rt' (but not 'rt' from words like 'art')). We'll do the same to the other text inputs in the background.

```{r silent_process, include=FALSE}
en_news <- stri_trans_general(en_news,"Latin-ASCII")
en_news <- gsub("[µºˆ_]+", '', en_news)
en_news <- char_tolower(en_news)
en_news <- gsub("(f|ht)tp(s?):\\/\\/(.*)[.][a-z=\\?\\/]+", "", en_news)
en_news <- gsub("via\\s*@[a-z_]{1,20}","", en_news)
en_news <- gsub("\\s@[a-z_]{1,20}", "", en_news)
en_news <- gsub("\\brt\\b", "", en_news)
en_news <- gsub("#[a-z0-9_]+", "", en_news)
en_news <- gsub(sprintf("(*UCP)\\b(%s)\\b", paste(badwords, collapse = "|")), "", en_news, perl = TRUE)
en_twitter <- stri_trans_general(en_twitter,"Latin-ASCII")
en_twitter <- gsub("[µºˆ_]+", '', en_twitter)
en_twitter <- char_tolower(en_twitter)
en_twitter <- gsub("(f|ht)tp(s?):\\/\\/(.*)[.][a-z=\\?\\/]+", "", en_twitter)
en_twitter <- gsub("via\\s*@[a-z_]{1,20}","", en_twitter)
en_twitter <- gsub("\\s@[a-z_]{1,20}", "", en_twitter)
en_twitter <- gsub("\\brt\\b", "", en_twitter)
en_twitter <- gsub("#[a-z0-9_]+", "", en_twitter)
en_twitter <- gsub(sprintf("(*UCP)\\b(%s)\\b", paste(badwords, collapse = "|")), "", en_twitter, perl = TRUE)
```

Now onto the next step, where we really get into the text analysis with `quanteda`.

```{r corpus}
#Create a 'Corpus' of words
corpus_blog <- corpus(en_blogs)
corpus_news <- corpus(en_news)
corpus_twitter <- corpus(en_twitter)

docnames(corpus_blog) <- paste('blog',1:ndoc(corpus_blog))
docnames(corpus_news) <- paste('news',1:ndoc(corpus_news))
docnames(corpus_twitter) <- paste('tweet',1:ndoc(corpus_twitter))

corpus_tot <- corpus_blog+corpus_news+corpus_twitter

dfm1 <- dfm(corpus_tot, removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE, removeURL = TRUE, removeTwitter = TRUE, removeHyphens=TRUE, ngrams=1)
dfm2 <- dfm(corpus_tot, removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE, removeURL = TRUE, removeTwitter = TRUE, removeHyphens=TRUE, ngrams=2)
dfm3 <- dfm(corpus_tot, removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE, removeURL = TRUE, removeTwitter = TRUE, removeHyphens=TRUE, ngrams=3)
dfm4 <- dfm(corpus_tot, removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE, removeURL = TRUE, removeTwitter = TRUE, removeHyphens=TRUE, ngrams=4)
```

We then turned the blog, news and twitter data into 'corpi', and assigned document names corresponding to each, then combined them to one 'corpus'. We then produced document feature matrices, or term document matrices, and in doing so breaking each document up into 1-, 2-, 3- and 4-gram chunks (e.g. for the string: "I want to go.", the 3-grams are "I want to" and "want to go."). While doing so, we cleaned out any numbers, punctuation, symbols, separators, and missed URLs or twitter crud.  With this, we can see which n-grams are the most popular! 

```{r clean0, include=FALSE}
rm(en_twitter, en_news, c_blogs, c_news, c_twitter, corpus_blog, corpus_news, corpus_twitter, badwords, badwords_url, blogs_lines, news_lines, twitter_lines, fraction)
gc()
```
```{r topfeatures}
topfeatures(dfm3)
```

Similarly, the single most common 4-gram from the set is "`r names(topfeatures(dfm4)[1])`". Seems a bit... pessimistic.

#Data Processing Options
Typical text processing begins to diverge from this point. Sometimes, you may be interested in removing all of the common words such as 'a', 'an', 'the', 'not', 'is' ... etc. These are called 'stop words', and are easily removed in the dfm creating step with `dfm(remove = stopwords())`. This won't work for us, as we want to know when to predict a stop word. Another manipulation to the text is called 'stemming', where words are shortened to their prefix base. For example, 'analyse', 'analysis', 'analysing', and 'analysed' would all be turned into 'analys', allowing for analysis not dependent on plurality, temporal (past/future/present) or other effects. Again, this would have been easy as `dfm(stem = TRUE)` but we want to predict the full word in our tool. 

There's a few other interesting analysis we can do with text data if we want:

##Sentiments
In terms of analysis, one can do 'sentiment analysis' and look at the texts, pulling out words that imply happiness, sadness, anger, etc. This can be done with the R package [`syuzhet`](https://cran.r-project.org/package=syuzhet) quite easily, so I'll demonstrate with the blogs data:
```{r sentiment}
library(syuzhet)
sentiments <- get_nrc_sentiment(en_blogs)
sentimentTotals <- data.frame("count" = colSums(sentiments))
sentimentTotals$sentiment <- rownames(sentimentTotals)
rownames(sentimentTotals) <- NULL

ggplot(data=sentimentTotals, aes(x=sentiment, y=count)) +
    geom_bar(aes(fill=sentiment), stat='identity') +
    theme_bw() + theme(legend.position = 'none', axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
    ggtitle("Sentiment Counts for Blogs") + xlab("Sentiment") + ylab("Total Count")
```
```{r clean1, include=FALSE}
rm(en_blogs, sentiments, sentimentTotals)
gc()
```

##Word Clouds
Similarly, word clouds are popular yet powerful visual tools for analyzing data. We'll make a word cloud for Twitter with the built in `textplot_wordcloud` function, which interfaces to the [`wordcloud`](https://cran.r-project.org/package=wordcloud) package. This works better after removing stop words, and stemming the tweets. I'll pull a few extra stop words out too, as some don't make it out due to our preprocessing removing punctuation or they're not in the base stop words list.

```{r wordcloud, warning=FALSE}
wordcloud_dfm<-dfm(corpus_tot, remove=stopwords('english'), stem=TRUE, removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE, removeSeparators = TRUE, removeURL = TRUE, removeTwitter = TRUE)

textplot_wordcloud(wordcloud_dfm, max.words = 100, random.order = FALSE, colors = RColorBrewer::brewer.pal(9, 'PuBuGn'))
```

```{r clean2, include=FALSE}
rm(wordcloud_dfm)
```

#N-Gram models
Back to the problem at hand. With our n-gram models, we'll want to be able to predict which words come next after a user has entered a few words. Things like 'I want to go to the ...' should result in proposed words such as 'store', 'park', 'mall'. We can do this by manipulating our n-gram data. 

We'll split our n-grams to the n-1 and 1-gram components. For a 4-gram "I want to go", that will be split to "I want to" and "go". We can then compare our counts of "go" to "eat" and "sleep" and any other combinations we saw in the corpus. The [`stringr`](https://cran.r-project.org/package=wordcloud) package will help here. I've made a function called 'splitGrams' that takes our document feature matrices as input, and provides a table of 'pregram' values and the expected word after each. 

```{r n_gram}
library(stringr)

splitGrams<-function(docfeatmatrix){
    freq_table<-colSums(docfeatmatrix)
    grams<-colnames(docfeatmatrix)
    grams<-trimws(gsub("_", " ", grams))
    n<-length(strsplit(grams[1], split = ' ')[[1]])
    if (n > 1){
        grams<-str_split_fixed(grams, ' ', n)
        gram_table<-data.frame(pregram=apply(grams, 1, function(x) paste(x[1:n-1], collapse = '_')), postgram=grams[,n], count=freq_table, stringsAsFactors = FALSE)
    }
    else if (n == 1){
        gram_table<-data.frame(pregram = rep(''), postgram=grams, count = freq_table, stringsAsFactors = FALSE)
    }
    else{
        stop("n-gram input not valid: < 1 n-gram length")
    }
    rm(grams, freq_table, n)
    #fix ordering
    gram_table<-gram_table[order(gram_table$pregram, -gram_table$count, gram_table$postgram),]
    rownames(gram_table)<-NULL
    return(gram_table)
}

gt1<-splitGrams(dfm1)
gt2<-splitGrams(dfm2)
gt3<-splitGrams(dfm3)
gt4<-splitGrams(dfm4)

```

With our gram popularity now calculated, we can look at our top for each n. For example, with our '3-gram' model, we see 'to the' yields:

```{r}
head(gt3[gt3$pregram == 'to_the', ])
```

Likewise, for our 'go to the ...' case, we get:

```{r} 
head(gt4[gt4$pregram == 'go_to_the', ])
```

These are low frequencies, but using more data should provide better results.

#Model Plan
With our ngrams, the plan is to produce predictions on what could follow, in much the same way that I have above. When the user provides 'go to the', the model should suggest 'gym', 'beach', 'hospital', etc. If a provided 'n-gram' has not been previously seen, or the count for a specific pregram is very very low, it may be beneficial to test the 'n-1'-gram, i.e., test 'to the' if 'go to the' does not provide enough hits.

#Note:
These make large data frames.

```{r}
library(pryr)
list(gt1=object_size(gt1), gt2=object_size(gt2), gt3=object_size(gt3), gt4=object_size(gt4))
```

However, they can be quickly cut down in size:

```{r}
list(gt2.small=object_size(gt2[gt2$count > 1, ]), gt3.small=object_size(gt3[gt3$count > 1, ]), gt4.small=object_size(gt4[gt4$count > 1, ]))
```

Or, removing only prgrams with only one postgram:

```{r}
g2unique<-data.frame(table(gt2$pregram))
g2unique<-g2unique[g2unique$Freq > 1,]
g3unique<-data.frame(table(gt3$pregram))
g3unique<-g3unique[g3unique$Freq > 1,]
g4unique<-data.frame(table(gt4$pregram))
g4unique<-g4unique[g4unique$Freq > 1,]

list(gt2.unique_pregram=object_size(gt2[gt2$pregram %in% g2unique$Var1,]), gt3.unique_pregram=object_size(gt3[gt3$pregram %in% g3unique$Var1,]), gt4.unique_pregram=object_size(gt4[gt4$pregram %in% g4unique$Var1,]))
```

It may be worth struggling through analysis of larger fractions of the text (25-100%) and keeping only count > 1, then building a 'parts of speech' model (eg suggest noun after an adjective, based on noun popularity). Backoff models will help with size reduction as well. Furthermore this would leave room for introduction of Proper Nouns (capitals) as well.



