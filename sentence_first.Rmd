---
title: "Sentence then token"
author: "Philip Bulsink"
date: '2017-04-07'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(quanteda)
library(stringr)
```

##Read in datat

```{r}
library(readr)
en_blogs<-read_lines("./data/en_US/en_US.blogs.txt", progress=FALSE)
en_news<-read_lines("./data/en_US/en_US.news.txt", progress=FALSE)
en_twitter<-read_lines("./data/en_US/en_US.twitter.txt", progress=FALSE)
blogs_lines<-length(en_blogs)
news_lines<-length(en_news)
twitter_lines<-length(en_twitter)
```

Split to sets: dev, training, test.
```{r}
set.seed(1)
fraction_dev<-0.05
fraction_training<-0.65
fraction_test<-0.30
blog_role<-sample(1:3, size = length(en_blogs), replace = TRUE, prob = c(fraction_dev, fraction_training, fraction_test))
news_role<-sample(1:3, size = length(en_blogs), replace = TRUE, prob = c(fraction_dev, fraction_training, fraction_test))
twitter_role<-sample(1:3, size = length(en_blogs), replace = TRUE, prob = c(fraction_dev, fraction_training, fraction_test))
blog_dev<-en_blogs[blog_role == 1]
news_dev<-en_news[news_role == 1]
twitter_dev<-en_twitter[twitter_role == 1]
```

Hack each to sentences.
```{r}
library(quanteda)
library(stringi)

blog_dev<-as.character(tokens(blog_dev, what='sentence', removeNumbers=TRUE, removePunc=TRUE, removeSymbols=TRUE, removeSeparators=TRUE, removeTwitter=TRUE, removeHyphens=TRUE, removeURL=TRUE))

news_dev<-as.character(tokens(news_dev, what='sentence', removeNumbers=TRUE, removePunc=TRUE, removeSymbols=TRUE, removeSeparators=TRUE, removeTwitter=TRUE, removeHyphens=TRUE, removeURL=TRUE))

twitter_dev<-as.character(tokens(twitter_dev, what='sentence', removeNumbers=TRUE, removePunc=TRUE, removeSymbols=TRUE, removeSeparators=TRUE, removeTwitter=TRUE, removeHyphens=TRUE, removeURL=TRUE))

dev_tot<-c(blog_dev, news_dev, twitter_dev)
#rm(blog_dev, news_dev, twitter_dev)

dev_tot<-stri_trans_general(dev_tot,"Latin-ASCII")
dev_tot <- gsub("[µºˆ_]+", '', dev_tot)

#set to lowercase
dev_tot <- char_tolower(dev_tot)

#Remove URLS
dev_tot <- gsub("(f|ht)tp(s?):\\/\\/(.*)[.][a-z=\\?\\/]+", "", dev_tot)

#Remove Twitter 'Via @username' and '@usernames' and 'rt' and hashtags
dev_tot <- gsub("via\\s*@[a-z_]{1,20}","", dev_tot)
dev_tot <- gsub("\\s@[a-z_]{1,20}", "", dev_tot)
dev_tot <- gsub("\\brt\\b", "", dev_tot)
dev_tot <- gsub("#[a-z0-9_]+", "", dev_tot)

#Cut punctuation/symbols/numbers
dev_tot <- gsub("[^a-zA-Z\\ ]", "", dev_tot)

#Get a 'bad words' list to remove from the corpus
badwords_url <- "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
badwords <- unlist(strsplit(RCurl::getURL(badwords_url), "\n"))
#last 'bad word' is an emoji, remove
badwords <- badwords[1:length(badwords)-1]
#replacement builder code from getAnywhere(tm::removeWords.character)
dev_tot <- gsub(sprintf("(*UCP)\\b(%s)\\b", paste(badwords, collapse = "|")), "", dev_tot, perl = TRUE)

#Remove multiple spaces from deleted words
dev_tot <- gsub("\\s\\s+", " ", dev_tot)

dfm1<-dfm(dev_tot, removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE, removeSeparators = TRUE, removeTwitter = TRUE, removeHyphens = TRUE, removeURL = TRUE, ngrams = 1)

#add start <S> and end <E> tags to the sentences.
dev_tot<-paste("^",dev_tot,"$", sep=" ")

dfm2<-dfm(dev_tot, removeNumbers = FALSE, removePunct = FALSE, removeSymbols = FALSE, removeSeparators = TRUE, removeHyphens = FALSE, removeURL = FALSE, ngrams = 2)
dfm3<-dfm(dev_tot, removeNumbers = FALSE, removePunct = FALSE, removeSymbols = FALSE, removeSeparators = TRUE, removeHyphens = FALSE, removeURL = FALSE, ngrams = 3)
dfm4<-dfm(dev_tot, removeNumbers = FALSE, removePunct = FALSE, removeSymbols = FALSE, removeSeparators = TRUE, removeHyphens = FALSE, removeURL = FALSE, ngrams = 4)
```

```{r n_gram}
library(stringr)

splitGrams<-function(docfeatmatrix){
    freq_table<-colSums(docfeatmatrix)
    grams<-colnames(docfeatmatrix)
    grams<-trimws(gsub("_", " ", grams))
    n<-length(strsplit(grams[1], split = ' ')[[1]])
    if (n > 1){
        grams<-str_split_fixed(grams, ' ', n)
        gram_table<-data.frame(pregram=apply(grams, 1, function(x) paste(x[1:n-1], collapse = '_')), postgram=grams[,n], count=freq_table, stringsAsFactors = FALSE)
    }
    else if (n == 1){
        gram_table<-data.frame(pregram = rep(''), postgram=grams, count = freq_table, stringsAsFactors = FALSE)
    }
    else{
        stop("n-gram input not valid: < 1 n-gram length")
    }
    rm(grams, freq_table, n)
    #fix ordering
    gram_table<-gram_table[order(gram_table$pregram, -gram_table$count, gram_table$postgram),]
    rownames(gram_table)<-NULL
    return(gram_table)
}

gt1<-splitGrams(dfm1)
gt2<-splitGrams(dfm2)
gt3<-splitGrams(dfm3)
gt4<-splitGrams(dfm4)

```
